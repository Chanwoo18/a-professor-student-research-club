{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/SSD/Utils"
      ],
      "metadata": {
        "id": "GZrqZyJXaH3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽Import libraries**"
      ],
      "metadata": {
        "id": "qgUU6byYgjd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from itertools import product\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "#utils folder에 있는 data agumentation.py에서 import \n",
        "from data_augmentation import *\n",
        "from match import *\n",
        "\n",
        "rootpath = '/content/drive/MyDrive/Colab Notebooks/SSD/Data'\n",
        "\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "              'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
        "              'sheep','sofa','train','tvmonitor']\n",
        "\n",
        "# SSD 300 설정\n",
        "\n",
        "ssd_cfg = {\n",
        "    'num_classes' : 21,\n",
        "    'input_size' : 300,\n",
        "    'bbox_aspect_num' : [4, 6, 6, 6, 4, 4], # 출력할 DBox 화면비 종류\n",
        "    'feature_maps' : [38, 19, 10, 5, 3, 1], # 각 source의 화상 크기\n",
        "    'steps' : [8, 16, 32, 64, 100, 300], # DBox 크기 결정\n",
        "    'min_sizes' : [30, 60, 111, 162, 213, 264], # DBox 크기 결정\n",
        "    'max_sizes' : [60, 111, 162, 213, 264, 315], # DBox 크기 결정\n",
        "    'aspect_ratios' : [[2], [2,3], [2,3], [2,3], [2], [2]],\n",
        "}"
      ],
      "metadata": {
        "id": "QUKvyAa4aa9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽데이터 경로 작성**\n",
        "    학습 및 검증용 이미지 데이터, 어노테이션 데이터의 파일 경로 리스트를 작성"
      ],
      "metadata": {
        "id": "bwCvL-92gvQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    데이터 경로를 저장한 리스트 작성\n",
        "    Parameters\n",
        "    ----------\n",
        "    rootpath : str\n",
        "        데이터 폴더의 경로\n",
        "    Returns\n",
        "    -------\n",
        "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "        데이터 경로 저장 리스트\n",
        "    \"\"\"\n",
        "\n",
        "    # 화상 파일과 어노테이션 파일의 경로 템플릿 작성\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
        "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
        "\n",
        "    # 훈련 및 검증 파일 ID 취득\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
        "\n",
        "    # 훈련 데이터의 화상파일과 어노테이션 파일의 경로 리스트 작성\n",
        "    train_img_list = list()\n",
        "    train_anno_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()  # 공백 및 줄바꿈 제거\n",
        "        img_path = (imgpath_template % file_id)  # 화상 경로\n",
        "        anno_path = (annopath_template % file_id)  # 어노테이션 경로\n",
        "        train_img_list.append(img_path)  # 리스트에 추가\n",
        "        train_anno_list.append(anno_path)  # 리스트에 추가\n",
        "\n",
        "    # 검증 데이터의 화상 파일과 어노테이션 파일의 경로 리스트 작성\n",
        "    val_img_list = list()\n",
        "    val_anno_list = list()\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()  # 공백과 줄바꿈 제거\n",
        "        img_path = (imgpath_template % file_id) # 화상 경로\n",
        "        anno_path = (annopath_template % file_id)  # 어노테이션 경로\n",
        "        val_img_list.append(img_path)  # 리스트에 추가\n",
        "        val_anno_list.append(anno_path)  # 리스트에 추가\n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
      ],
      "metadata": {
        "id": "0fV-cfnx_di5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽Anno_xml2list**\n",
        "    XML 형식의 어노테이션을 리스트 형식으로 변환하는 클래스"
      ],
      "metadata": {
        "id": "nsZ899RYg71w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Anno_xml2list(object):\n",
        "    \"\"\"\n",
        "    한 화상의 XML 형식 어노테이션 데이터를 화상 크기로 규격화하여 리스트 형식으로 변환\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    classes : 리스트\n",
        "        VOC의 클래스명을 저장한 클래스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes):\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path, width, height):\n",
        "        \"\"\"\n",
        "        한 화상의 XML 형식 어노테이션 데이터를 화상 크기로 규격화하여 리스트 형식으로 변환\n",
        "        Parameters\n",
        "        ----------\n",
        "        xml_path : str\n",
        "            xml 파일 경로\n",
        "        width : int\n",
        "            대상 화상 폭\n",
        "        height : int\n",
        "            대상 화상 높이\n",
        "        Returns\n",
        "        -------\n",
        "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
        "            물체의 어노테이션 데이터를 저장한 리스트. 화상에 존재하는 물체 수만큼의 요소를 가짐.\n",
        "        \"\"\"\n",
        "\n",
        "        # 화상 내 모든 물체의 어노테이션을 이 리스트에 저장\n",
        "        ret = []\n",
        "\n",
        "        # XML 파일 로드\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "\n",
        "        # 화상 내 물체의 수 만큼 반복\n",
        "        for obj in xml.iter('object'):\n",
        "\n",
        "            # annotation에서 검지가 difficult로 설정된 것은 제외\n",
        "            difficult = int(obj.find('difficult').text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "\n",
        "            # 한 물체의 어노테이션을 저장하는 리스트\n",
        "            bndbox = []\n",
        "\n",
        "            name = obj.find('name').text.lower().strip()  # 물체 이름\n",
        "            bbox = obj.find('bndbox')  # 바운딩 박스 정보\n",
        "\n",
        "            # 어노테이션의 xmin, ymin, xmax, ymax를 취득하고, 0 ~ 1로 규격화\n",
        "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "            for pt in (pts):\n",
        "                # VOC는 원점이 (1, 1)이므로 1을 빼서 (0, 0)으로 한다.\n",
        "                cur_pixel = int(bbox.find(pt).text) - 1\n",
        "\n",
        "                # 폭, 높이로 규격화\n",
        "                if pt == 'xmin' or pt == 'xmax':  # x 방향의 경우 폭으로 나눔.\n",
        "                    cur_pixel /= width\n",
        "                else:  # y방향의 경우 높이로 나눔.\n",
        "                    cur_pixel /= height\n",
        "\n",
        "                bndbox.append(cur_pixel)\n",
        "\n",
        "            # 어노테이션 클래스명 index를 취득하여 추가\n",
        "            label_idx = self.classes.index(name)\n",
        "            bndbox.append(label_idx)\n",
        "\n",
        "            # res에 [xmin, ymin, xmax, ymax, label_ind]를 더한다.\n",
        "            ret += [bndbox]\n",
        "\n",
        "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]"
      ],
      "metadata": {
        "id": "uldAW6ZslxmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "              'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
        "              'sheep','sofa','train','tvmonitor']\n",
        "\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "\n",
        "# 화상 로드용으로 Opencv 사용\n",
        "ind = 1\n",
        "img_file_path = val_img_list[ind]\n",
        "img = cv2.imread(img_file_path) # [높이][폭][색BGR]\n",
        "height, width, channels = img.shape # 화상 크기 취득\n",
        "\n",
        "# 어노테이션을 리스트로 표시\n",
        "transform_anno(val_anno_list[ind], width, height)"
      ],
      "metadata": {
        "id": "PpFVfwM5fIG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽DataTransform**\n",
        "    이미지와 어노테이션 전처리를 실시하는 클래스"
      ],
      "metadata": {
        "id": "5ucLuxdxkIph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    화상과 어노테이션의 전처리 클래스. 훈련과 추론에서 다르게 작동.\n",
        "    화상 크기를 300*300으로 한다.\n",
        "    학습 시 데이터 확장을 수행.\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_size : int\n",
        "        리사이즈 대상 화상의 크기\n",
        "    color_mean : (B, G, R)\n",
        "        각 색상채널 평균 값\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),  # int를 float32로 변환\n",
        "                ToAbsoluteCoords(),  # 어노테이션 데이터의 규격화 반환\n",
        "                PhotometricDistort(),  # 화상의 색조 등 임의 변화\n",
        "                Expand(color_mean),  # 화상 캔버스 확대\n",
        "                RandomSampleCrop(),  # 화상 내 특정 부분 무작위 추출\n",
        "                RandomMirror(),  # 화상 반전\n",
        "                ToPercentCoords(),  # 어노테이션 데이터를 0 ~ 1로 규격화\n",
        "                Resize(input_size),  # 화상 크기를 input_size*input_size로 변형\n",
        "                SubtractMeans(color_mean)  # BGR 색상 평균값 빼기\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),  # int를 float32로 변환\n",
        "                Resize(input_size),  # 화상 크기를 input_size * input_size로 변환\n",
        "                SubtractMeans(color_mean)  # BGR 색상 평균값 빼기\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            전처리 모드 지정\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img, boxes, labels)"
      ],
      "metadata": {
        "id": "IRBYGv5BfM6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "\n",
        "# 1. 화상 읽기\n",
        "image_file_path = train_img_list[0]\n",
        "img = cv2.imread(img_file_path) # [높이][폭][색BGR]\n",
        "height, width, channels = img.shape # 화상 크기 취득\n",
        "\n",
        "# 2. 어노테이션을 리스트로\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "anno_list = transform_anno(train_anno_list[0], width, height)\n",
        "\n",
        "# 3. 원본 표시\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 4. 전처리 클래스 작성\n",
        "color_mean = (104, 117 ,123) # (bgr) 색상의 평균값\n",
        "input_size = 300\n",
        "transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "# 5. Train 화상 표시\n",
        "phase = 'train'\n",
        "img_transformed, boxes, labels = transform(\n",
        "                                    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 6. Val 화상 표시\n",
        "phase = 'val'\n",
        "img_transformed, boxes, labels = transform(\n",
        "                                    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "15n48HtmfPyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽VOCDataset**\n",
        "    VOC2012 데이터셋 작성"
      ],
      "metadata": {
        "id": "MTMMsNA6kQqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    VOC2012의 데이터셋을 작성하는 클래스. 파이토치의 dataset 클래스를 상속\n",
        "    ----------\n",
        "    img_list : 리스트\n",
        "        화상 경로를 저장한 리스트\n",
        "    anno_list : 리스트\n",
        "        어노테이션 경로를 저장한 리스트\n",
        "    phase : 'train' or 'test'\n",
        "        학습 또는 훈련 설정\n",
        "    transform : object\n",
        "        전처리 클래스 인스턴스\n",
        "    transform_anno : object\n",
        "        xml 어노테이션을 리스트로 변환하는 인스턴스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase  # train 또는 val 지정\n",
        "        self.transform = transform  # 화상 변형\n",
        "        self.transform_anno = transform_anno  # 어노테이션 데이터를 xml에서 리스트로 변경\n",
        "\n",
        "    def __len__(self):\n",
        "        '''화상의 매수 반환'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        전처리한 화상의 텐서 형식 데이터와 어노테이션 취득\n",
        "        '''\n",
        "        im, gt, h, w = self.pull_item(index)\n",
        "        return im, gt\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''전처리한 화상의 텐서 형식 데이터, 어노테이션, 화상의 높이, 폭 취득'''\n",
        "\n",
        "        # 1. 화상 읽기\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [높이][폭][색BGR]\n",
        "        height, width, channels = img.shape  # 화상 크기 취득\n",
        "\n",
        "        # 2. xml 형식의 어노테이션 정보를 리스트에 저장\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
        "\n",
        "        # 3. 전처리 실시\n",
        "        img, boxes, labels = self.transform(\n",
        "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
        "\n",
        "        # 색상 채널으 ㅣ순서가 BGR이므로 RGB로 순서 변경\n",
        "        # (높이, 폭, 색상 채널)의 순서를 (색상 채널, 높이, 폭)으로 변경\n",
        "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        # BBox와 라벨을 세트로 한 np.array 작성. 변수 이름 gt는 ground truth의 의미\n",
        "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width"
      ],
      "metadata": {
        "id": "Otw4XkfmfRRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "color_mean = (104, 117, 123) # BGR 평균값\n",
        "input_size = 300 # 화상의 input 사이즈를 300*300으로 함.\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train',\n",
        "                transform=DataTransform(input_size, color_mean),\n",
        "                transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase='train',\n",
        "                transform=DataTransform(input_size, color_mean), \n",
        "                transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "# 데이터 출력 예\n",
        "val_dataset.__getitem__(1)"
      ],
      "metadata": {
        "id": "c3OB7VCwfUwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽old_collate_fn**\n",
        "    데이터 로더를 구현"
      ],
      "metadata": {
        "id": "wiMjZ0bAkXF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def od_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Dataset에서 꺼내는 어노테이션 데이터의 크기는 화상마다 다르다.\n",
        "    화상 내의 물체 수가 두개이면 (2, 5)사이즈이지만, 세 개이면 (3, 5) 등으로 바뀐다.\n",
        "    변화에 대응하는 DataLoader를 만드는 collate_fn을 작성한다.\n",
        "    collate_fn은 파이토치 리스트로 mini batch를 작성하는 함수이디ㅏ.\n",
        "    미니 배치 분량 화상이 나열된 리스트 변수 batch에 미니 배치 번호를 지정하는 \n",
        "    차원을 가장 앞에 하나 추가하여 리스트 형태를 변형한다.\n",
        "    \"\"\"\n",
        "\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0]은 화상 gt\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1]은 어노테이션 gt\n",
        "\n",
        "    # imgs는 미니배치 크기으 ㅣ리스트\n",
        "    # 리스트 요소는 torch.Size([3, 300, 300])\n",
        "    # 이 리스트를 torch.Size([batch_num, 3, 300, 300])의 텐서로 변환\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # targets은 어노테이션의 정답인 gt 리스트\n",
        "    # 리스트 크기 = 미니 배치 크기\n",
        "    # targets 리스트의 요소는 [n, 5] \n",
        "    # n은 화상마다 다르며 화상 속 물체의 수\n",
        "    # 5는 [xmin, ymin, xmax, ymax, class_index]\n",
        "\n",
        "    return imgs, targets"
      ],
      "metadata": {
        "id": "mXIEDzXlfWst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로더 작성\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "                        train_dataset, \n",
        "                        batch_size =batch_size, shuffle=True, \n",
        "                        collate_fn=od_collate_fn)\n",
        "\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "                        val_dataset, \n",
        "                        batch_size =batch_size, shuffle=True, \n",
        "                        collate_fn=od_collate_fn)\n",
        "\n",
        "# 사전형 변수에 정리\n",
        "dataloaders_dict = {'train' : train_dataloader,\n",
        "                    'val' : val_dataloader}\n",
        "\n",
        "# 동작 확인\n",
        "batch_iterator = iter(dataloaders_dict['val']) # 반복자로 변환\n",
        "images, targets = next(batch_iterator) # 첫번째 요소 추출\n",
        "print(images.size()) # torch.Size([4, 3, 300, 300])\n",
        "print(len(targets))\n",
        "print(targets[1].size()) # 미니 배치 크기 리스트, 각 요소는 [n, 5] , n은 물체 수"
      ],
      "metadata": {
        "id": "mtdWnAbRfZ09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽make_vgg**\n",
        "    VGG 모듈을 작성"
      ],
      "metadata": {
        "id": "H0fbYE1ykc6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 34층에 걸친 vgg 모듈을 작성\n",
        "\n",
        "def make_vgg():\n",
        "    layers = []\n",
        "    in_channels = 3  # 색 채널 수\n",
        "\n",
        "    # vgg 모듈에서 사용하는 합성곱 층이나 최대 풀링 채널 수 \n",
        "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,\n",
        "           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\n",
        "\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif v == 'MC':\n",
        "            # ceil은 계산 결과(float)에서 출력 크기의 소수점을 올려 정수로 하는 모드\n",
        "            # default는 계산 결과(float)에서 출력 크기의 소수점을 버려 정수로 하는 floor 모드\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            # ReLU 인수 inplace는 ReLU에 대한 입력을 메모리 상에 유지할 것인지, 혹은\n",
        "            # 입력을 재작성 하여 출력으로 바꾼 후 메모리상에 유지하지 않을 것인지를 나타냄.\n",
        "            # inplace=True 입력 시 메모리상에 입력을 유지하지 않고, 입력을 재작성 (메모리 절약됨)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "\n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6,\n",
        "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "    return nn.ModuleList(layers)"
      ],
      "metadata": {
        "id": "YTPPV3DXfaoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "vgg_test = make_vgg()\n",
        "print(vgg_test)"
      ],
      "metadata": {
        "id": "5GP8jGnHfdOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽make_extras**\n",
        "    extra 모듈 구현\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T1PPQwZWk0fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8층에 걸친 extras 모듈ㅇ르 작성\n",
        "\n",
        "def make_extras():\n",
        "    layers = []\n",
        "    in_channels = 1024  # vgg모듈에서 출력된 extra에 입력되는 화상 채널 수 \n",
        "\n",
        "    # extra 모듈의 합성곱 층 채널 수를 설정하는 구성(configuration)\n",
        "    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\n",
        "\n",
        "    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\n",
        "    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\n",
        "\n",
        "    \n",
        "# 활성화 함수의 ReLU는 SSD 모듈의 순전파에서 준비하고, \n",
        "# extra에서는 준비하지 않음.\n",
        "    \n",
        "    return nn.ModuleList(layers)"
      ],
      "metadata": {
        "id": "_pRSrCrHfejo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "extras_test = make_extras()\n",
        "print(extras_test)"
      ],
      "metadata": {
        "id": "2Ldyy3EnfgTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽make_extras**\n",
        "    loc 및 conf 모듈을 구현"
      ],
      "metadata": {
        "id": "bPgJBlBtk68T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디폴트 박스의 오프셋을 출력하는 loc_layers와\n",
        "# 디폴트 박스 각 클래스 신뢰도 confidence를 출력하는 conf_layers 작성\n",
        "\n",
        "def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
        "\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "\n",
        "    # VGG의 22층, conv4_3(source1)의 합성곱 층\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # VGG의 최종층(source2)의 합성곱 층\n",
        "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extra(source3)의 합성곱 층\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extra（source4)의 합성곱 층\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extra（source5）의 합성곱 층\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extra（source6）의 합성곱 층\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)"
      ],
      "metadata": {
        "id": "7MuSLZUmfiMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "loc_test, conf_test = make_loc_conf()\n",
        "print(loc_test)\n",
        "print(conf_test)"
      ],
      "metadata": {
        "id": "QFYpFbZofkox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽L2Norm**\n",
        "    conv4_3에서 출력에 적용하는 L2Norm층을 구현\n"
      ],
      "metadata": {
        "id": "8bc6s-oPk__d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convC4_3의 출력을 scale=20의 L2Norm으로 정규화하는 층\n",
        "\n",
        "class L2Norm(nn.Module):\n",
        "    def __init__(self, input_channels=512, scale=20):\n",
        "        super(L2Norm, self).__init__()  #  부모 클래스의 생성자 실행\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_channels))\n",
        "        self.scale = scale  # 계수 weight의 초깃값으로 설정할 값\n",
        "        self.reset_parameters()  # 파라미터 초기화\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        '''결합 파라미터의 scale 크기 값으로 초기화를 실행'''\n",
        "        init.constant_(self.weight, self.scale)  # weight 값이 모두 scale(=20)이 된다.\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''38*38의 특징량에 대해 512 채널에 걸쳐 제곱합의 루트를 구했다.\n",
        "        38*38개의 값을 사용하여 각 특징량을 정규화한 후 계수를 곱하여 계산하는 층'''\n",
        "\n",
        "        # 각 채널의 38*38개 특징량의 채널 방향 제곱합을 계산하고\n",
        "        # 루트를 구해 나누어 정규화한다.\n",
        "        # norm의 텐서 사이즈는 torch.Size([batch_num, 1, 38, 38])\n",
        "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
        "        x = torch.div(x, norm)\n",
        "\n",
        "        # 계수를 곱한다. 계수는 채널마다 하나로, 512개의 계수를 갖는다.\n",
        "        # self.weight의 텐서 사이즈는 torch.Size([512])로,\n",
        "        # torch.Size([batch_num, 512, 38, 38])까지 변형한다.\n",
        "        weights = self.weight.unsqueeze(\n",
        "            0).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        out = weights * x\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "X5JoClU-fmSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽DBox**\n",
        "    디폴트 박스 구현"
      ],
      "metadata": {
        "id": "47-UvLnFlggm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디폴트 박스를 출력하는 클래스\n",
        "\n",
        "\n",
        "class DBox(object):\n",
        "    def __init__(self, cfg):\n",
        "        super(DBox, self).__init__()\n",
        "\n",
        "        # 초기 설정\n",
        "        self.image_size = cfg['input_size']  # 화상 크기는 300\n",
        "        # [38, 19, …] 각 source의 특징량 맵 크기\n",
        "        self.feature_maps = cfg['feature_maps']\n",
        "        self.num_priors = len(cfg[\"feature_maps\"])  # source 개수 = 6\n",
        "        self.steps = cfg['steps']  # [8, 16, …] DBox의 픽셀 크기\n",
        "\n",
        "        self.min_sizes = cfg['min_sizes']\n",
        "        # [30, 60, …] 작은 정사각형의 DBox 픽셀 크기(정확히는 면적)\n",
        "\n",
        "        self.max_sizes = cfg['max_sizes']\n",
        "        # [60, 111, …] 큰 정사각형의 DBox 픽셀 크기(정확히는 면적)\n",
        "\n",
        "        self.aspect_ratios = cfg['aspect_ratios']  # 정사각형의 DBox 화면비(종횡비)\n",
        "\n",
        "    def make_dbox_list(self):\n",
        "        '''DBox 작성'''\n",
        "        mean = []\n",
        "        # 'feature_maps': [38, 19, 10, 5, 3, 1]\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            for i, j in product(range(f), repeat=2):  # f 까지의 수로 두 쌍의 조합을 작성. f_p_2개\n",
        "                \n",
        "                # 특징량의 화상 크기\n",
        "                # 300 / 'steps': [8, 16, 32, 64, 100, 300],\n",
        "                f_k = self.image_size / self.steps[k]\n",
        "\n",
        "                # DBox의 중심 좌표 x,y. 0~1 로 정규화되어 있다.\n",
        "                cx = (j + 0.5) / f_k\n",
        "                cy = (i + 0.5) / f_k\n",
        "\n",
        "                # 화면비 1의 작은 DBox [cx,cy, width, height]\n",
        "                # 'min_sizes': [30, 60, 111, 162, 213, 264]\n",
        "                s_k = self.min_sizes[k]/self.image_size\n",
        "                mean += [cx, cy, s_k, s_k]\n",
        "\n",
        "                # 화면비 1의 큰 DBox [cx,cy, width, height]\n",
        "                # 'max_sizes': [60, 111, 162, 213, 264, 315],\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
        "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
        "\n",
        "                # 그 오 ㅣ화면비의 defBox [cx,cy, width, height]\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), s_k*sqrt(ar)]\n",
        "\n",
        "        # DBox를 텐서로 변환 torch.Size([8732, 4])\n",
        "        output = torch.Tensor(mean).view(-1, 4)\n",
        "\n",
        "        # DBox가 화상 밖으로 돌출되는 것을 막기 위해 크기를 최소 0, 최대 1로 한다.\n",
        "        output.clamp_(max=1, min=0)\n",
        "        return output"
      ],
      "metadata": {
        "id": "lzLlaThlfoVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "\n",
        "# DBox 작성\n",
        "dbox = DBox(ssd_cfg)\n",
        "dbox_list = dbox.make_dbox_list()\n",
        "\n",
        "# dbox 출력 확인 \n",
        "pd.DataFrame(dbox_list.numpy())"
      ],
      "metadata": {
        "id": "g4NTVx2Bfq1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽SSD**\n",
        "    순전파 계산을 실행하여 ssd 클래스 구현"
      ],
      "metadata": {
        "id": "-xoBsGWElkK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SSD클래스 작성\n",
        "\n",
        "\n",
        "class SSD(nn.Module):\n",
        "\n",
        "    def __init__(self, phase, cfg):\n",
        "        super(SSD, self).__init__()\n",
        "\n",
        "        self.phase = phase  # train or inference\n",
        "        self.num_classes = cfg[\"num_classes\"]  # 클래스 수=21\n",
        "\n",
        "        # SSD 네트워크 작성\n",
        "        self.vgg = make_vgg()\n",
        "        self.extras = make_extras()\n",
        "        self.L2Norm = L2Norm()\n",
        "        self.loc, self.conf = make_loc_conf(\n",
        "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
        "\n",
        "        # DBox 작성\n",
        "        dbox = DBox(cfg)\n",
        "        self.dbox_list = dbox.make_dbox_list()\n",
        "\n",
        "        # 추론 시 Detect 클래스 준비\n",
        "        if phase == 'inference':\n",
        "            self.detect = Detect()\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = list()  # loc와 conf에 입력 source1 ~ 6 저장\n",
        "        loc = list()  # loc의 출력 저장\n",
        "        conf = list()  # conf의 출력 저장\n",
        "\n",
        "        # vgg의 conv4_3까지 계산\n",
        "        for k in range(23):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        # conv4_3의 출력을 L2Norm에 입력하고, source1을 작성하여 sources에 추가\n",
        "        source1 = self.L2Norm(x)\n",
        "        sources.append(source1)\n",
        "\n",
        "        # vgg를 마지막까지 계산하여 source2를 작성하고 sources에 추가\n",
        "        for k in range(23, len(self.vgg)):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        sources.append(x)\n",
        "\n",
        "        # extras의 conv와 ReLU 계산\n",
        "        # source3～6을 sources에 추가\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = F.relu(v(x), inplace=True)\n",
        "            if k % 2 == 1:  # conv→ReLU→cov→ReLU를 하여 source에 넣는다.\n",
        "                sources.append(x)\n",
        "\n",
        "        # source1 ~ 6 에 각각 대응하는 합성곱을 1회씩 적용\n",
        "        # zip으로 for 루프의 여러 리스트 요소 취득\n",
        "        # source1 ~ 6 까지 있어 루프가 6회 실시\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            # Permute로 요소의 순서를 교체\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "            # l(x), c(x)로 합성곱 실행\n",
        "            # l(x), c(x)으 ㅣ출력 크기는 [batch_num, 4*화면비의 종류 수 , featuremap높이, featuremap폭]\n",
        "            # source에 따라 화면비으 ㅣ종류 수가 다르며, 번거로워 순서를 바꾸어서 조정\n",
        "            # permute로 요소 순서를 당므과 같이 교체 \n",
        "            # [minibatch 수 , featuremap 수 , featuremap 수 ,4*화면비의 종류 수]\n",
        "            # torch.contiguous()은 메모리 상에 연속적으로 요소를 배치하는 명령\n",
        "            # 이후 view 함수 사용\n",
        "            # view를 수행하므로 대상의 변수가 메모리 상에 연속적으로 배치되어야 한다.\n",
        "\n",
        "        # loc와 conf의 모양 변형\n",
        "        # loc의 크기는 torch.Size([batch_num, 34928])\n",
        "        # conf의 크기는 torch.Size([batch_num, 183372])가 된다.\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
        "\n",
        "        # loc와 conf의 모양 조정\n",
        "        # lloc의 크기는 torch.Size([batch_num, 8732, 4])\n",
        "        # conf의 크기는 torch.Size([batch_num, 8732, 21])\n",
        "        loc = loc.view(loc.size(0), -1, 4)\n",
        "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
        "\n",
        "        # 마지막으로 출력\n",
        "        output = (loc, conf, self.dbox_list)\n",
        "\n",
        "        if self.phase == \"inference\":  #  추론 시\n",
        "            # detect 클래스의 forward 실행\n",
        "            # 반환 값의 크기는 torch.Size([batch_num, 21, 200, 5])\n",
        "            return self.detect(output[0], output[1], output[2])\n",
        "\n",
        "        else:  # 학습 시\n",
        "            return output\n",
        "            # 반환 값은 (loc, conf, dbox_list)의 튜플"
      ],
      "metadata": {
        "id": "9jh7EEZifsSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽Decode**\n",
        "    decode 함수 구현"
      ],
      "metadata": {
        "id": "NJhSuWSDlrDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(loc, dbox_list):\n",
        "    \"\"\"\n",
        "    오프셋 정보로  DBox를 BBox로 변환한다.\n",
        "    Parameters\n",
        "    ----------\n",
        "    loc:  [8732,4]\n",
        "        SSD 모델로 추론하는 오프셋 정보\n",
        "    dbox_list: [8732,4]\n",
        "        DBox 정보\n",
        "    Returns\n",
        "    -------\n",
        "    boxes : [xmin, ymin, xmax, ymax]\n",
        "        BBox 정보\n",
        "    \"\"\"\n",
        "\n",
        "    # DBox는 [cx, cy, width, height]로 저장되었다.\n",
        "    # loc도 [Δcx, Δcy, Δwidth, Δheight]로 저장되었다.\n",
        "\n",
        "    # 오프셋 정보로 BBox를 구한다.\n",
        "    boxes = torch.cat((\n",
        "        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n",
        "        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\n",
        "    # boxes의 크기는 torch.Size([8732, 4])가 된다.\n",
        "\n",
        "    # BBox의 좌표 정보를 [cx, cy, width, height]에서 [xmin, ymin, xmax, ymax] 로 변경\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2  # 좌표 (xmin,ymin)로 변환\n",
        "    boxes[:, 2:] += boxes[:, :2]  # 좌표 (xmax,ymax)로 변환\n",
        "\n",
        "    return boxes"
      ],
      "metadata": {
        "id": "gVmOPA8yfvGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽nm_suppression**\n",
        "### **Non-maximum Suppression 실시 함수 구현**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    미리 8,732개의 DBox를 준비하여 물체를 감지하므로, BBox를 계산하면 화상 속 동일한 물체에 다른 BBox가 조금 다르게 복수 피팅 될 때가 있다. \n",
        "    겹치는 BBox를 삭제하고 하나의 물체에 하나의 BBox만 남기는 처리를 Non-Maximum Suppression 이라고 한다."
      ],
      "metadata": {
        "id": "4FT5DMA_lv6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-Maximum Suppression을 실시하는 함수\n",
        "\n",
        "\n",
        "def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\n",
        "    \"\"\"\n",
        "    Non-Maximum Suppression을 실시하는 함수\n",
        "    boxes중 겹치는（overlap이상) BBox 삭제\n",
        "    Parameters\n",
        "    ----------\n",
        "    boxes : [신뢰도 임곗값（0.01)을 넘은 BBox 수,4]\n",
        "        BBox 정보\n",
        "    scores :[신뢰도 임곗값（0.01)을 넘은 BBox 수]\n",
        "        conf 정보\n",
        "    Returns\n",
        "    -------\n",
        "    keep : 리스트\n",
        "        conf의 내림차순으로 nms를 통과한 index를 저장\n",
        "    count：int\n",
        "        nms를 통과한 BBox 수\n",
        "    \"\"\"\n",
        "\n",
        "    # return 모형 작성\n",
        "    count = 0\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\n",
        "    # keep：torch.Size([신뢰도 임곗값을 넘은 BBox 수)], 요소는 전부 0\n",
        "\n",
        "    # 각 BBox의 면적 area 계산\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    area = torch.mul(x2 - x1, y2 - y1)\n",
        "\n",
        "    # boxes 복사. 나중에 BBox 중복도(IOU) 계산 시 모형으로 준비\n",
        "    tmp_x1 = boxes.new()\n",
        "    tmp_y1 = boxes.new()\n",
        "    tmp_x2 = boxes.new()\n",
        "    tmp_y2 = boxes.new()\n",
        "    tmp_w = boxes.new()\n",
        "    tmp_h = boxes.new()\n",
        "\n",
        "    # scores를 오름차순으로 나열\n",
        "    v, idx = scores.sort(0)\n",
        "\n",
        "    # 상위 top_k개(200개)의 BBox index를 꺼낸다(200개가 존재하지 않는 경우도 있음)\n",
        "    idx = idx[-top_k:]\n",
        "\n",
        "    # idx의 요소 수가 0이 아닌 한 루프한다.\n",
        "    while idx.numel() > 0:\n",
        "        i = idx[-1]  # conf의 최대 index를 i로 지정\n",
        "\n",
        "        # keep의 끝에 conf 최대 index 저장\n",
        "        # 이 index의 BBox와 크게 겹치는 BBox를 삭제\n",
        "        keep[count] = i\n",
        "        count += 1\n",
        "\n",
        "        # 마지막 BBox는 루프를 빠져나옴\n",
        "        if idx.size(0) == 1:\n",
        "            break\n",
        "\n",
        "        # 현재 conf 최대의 index를 keep에 저장했으므로 idx를 하나 감소시킴\n",
        "        idx = idx[:-1]\n",
        "\n",
        "        # -------------------\n",
        "        # 지금부터 keep에 저장한 BBox와 크게 겹치는 BBox를 추출하여 삭제\n",
        "        # -------------------\n",
        "        # 하나 감소시킨 idx까지의 BBox를 out으로 지정한 변수로 작성\n",
        "        torch.index_select(x1, 0, idx, out=tmp_x1)\n",
        "        torch.index_select(y1, 0, idx, out=tmp_y1)\n",
        "        torch.index_select(x2, 0, idx, out=tmp_x2)\n",
        "        torch.index_select(y2, 0, idx, out=tmp_y2)\n",
        "\n",
        "        # 모든 BBox를 현재 BBox=index가 i로 겹치는 값까지로 설정(clamp)\n",
        "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\n",
        "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
        "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
        "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\n",
        "\n",
        "        # w와 h의 텐서 크기를 index 하나 줄인 것으로 한다.\n",
        "        tmp_w.resize_as_(tmp_x2)\n",
        "        tmp_h.resize_as_(tmp_y2)\n",
        "\n",
        "        # clamp한 상태에서 BBox의 폭과 높이를 구한다.\n",
        "        tmp_w = tmp_x2 - tmp_x1\n",
        "        tmp_h = tmp_y2 - tmp_y1\n",
        "\n",
        "        # 폭이나 높이가 음수인 것은 0으로 한다.\n",
        "        tmp_w = torch.clamp(tmp_w, min=0.0)\n",
        "        tmp_h = torch.clamp(tmp_h, min=0.0)\n",
        "\n",
        "        # clamp된 상태 면적을 구한다.\n",
        "        inter = tmp_w*tmp_h\n",
        "\n",
        "        # IoU = intersect 부분 / (area(a) + area(b) - intersect 부분) 계산\n",
        "        rem_areas = torch.index_select(area, 0, idx)  # 각 BBox의 원래 면적\n",
        "        union = (rem_areas - inter) + area[i]  # 두 구역의 합(OR) 면적\n",
        "        IoU = inter/union\n",
        "\n",
        "        # IoU가 overlap보다 작은 idx만 남긴다\n",
        "        idx = idx[IoU.le(overlap)]  # le은 Less than or Equal to 처리를 하는 연산\n",
        "        # IoU가 overlap보다 큰 idx는 처음 선택한 keep에 저장한 idx와 동일한 물체에 BBox를 둘러싸고 있어 삭제\n",
        "    \n",
        "    # while 루프에서 빠져나오면 종료\n",
        "\n",
        "    return keep, count"
      ],
      "metadata": {
        "id": "NSL7mm06fyIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽Detect**\n",
        "    detect 클래스 구현"
      ],
      "metadata": {
        "id": "tIUs12E-mRhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SSD 추론 시 conf와 loc의 출력에서 중복을 제거한 BBox 출력\n",
        "\n",
        "\n",
        "class Detect(Function):\n",
        "\n",
        "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\n",
        "        self.softmax = nn.Softmax(dim=-1)  \n",
        "        # conf를 소프트맥스 함수로 정규화하기 위해 준비\n",
        "        self.conf_thresh = conf_thresh  \n",
        "        # conf가 conf_thresh=0.01보다 높은 DBox만 취급\n",
        "        self.top_k = top_k  \n",
        "        # conf가 높은 top_k개를 nm_supression으로 계산에 사용하는 top_k = 200\n",
        "        self.nms_thresh = nms_thresh  \n",
        "        # nm_supression으로 IOU가 nms_thresh=0.45보다 크면 동일한 물체의 BBox로 간주\n",
        "\n",
        "    def forward(self, loc_data, conf_data, dbox_list):\n",
        "        \"\"\"\n",
        "        순전파 계산 실행\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        loc_data:  [batch_num,8732,4]\n",
        "            오프셋 정보\n",
        "        conf_data: [batch_num, 8732,num_classes]\n",
        "            감지 신뢰도\n",
        "        dbox_list: [8732,4]\n",
        "            DBox 정보\n",
        "        Returns\n",
        "        -------\n",
        "        output : torch.Size([batch_num, 21, 200, 5])\n",
        "            （batch_num, 클래스, conf의 top200, BBox 정보）\n",
        "        \"\"\"\n",
        "\n",
        "        # 각 크기 취득\n",
        "        num_batch = loc_data.size(0)  # 미니 배치 크기\n",
        "        num_dbox = loc_data.size(1)  # DBox의 수 = 8732\n",
        "        num_classes = conf_data.size(2)  # 클래스 수 = 21\n",
        "\n",
        "        # conf는 소프트맥스를 적용하여 정규화\n",
        "        conf_data = self.softmax(conf_data)\n",
        "\n",
        "        # 출력 형식을 작성. 텐서 크기 [minibatch 수, 21, 200, 5]\n",
        "        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\n",
        "\n",
        "        # cof_data 순서를 [batch_num,8732,num_classes]에서 [batch_num, num_classes,8732]로 변경\n",
        "        conf_preds = conf_data.transpose(2, 1)\n",
        "\n",
        "        # 미니 배치마다 루프\n",
        "        for i in range(num_batch):\n",
        "\n",
        "            # 1. loc와 DBox로 수정한 BBox [xmin, ymin, xmax, ymax] 를 구한다\n",
        "            decoded_boxes = decode(loc_data[i], dbox_list)\n",
        "\n",
        "            # conf의 복사본 작성\n",
        "            conf_scores = conf_preds[i].clone()\n",
        "\n",
        "            # 화상 클래스별 루프(배경 클래스의 index인 0은 계산하지 않고 index=1부터）\n",
        "            for cl in range(1, num_classes):\n",
        "\n",
        "                # 2.conf의 임곗값을 넘은 BBox를 꺼낸다.\n",
        "                # conf의 임곗값을 넘고 있는지 마스크를 작성하여\n",
        "                # 임곗값을 넘은 conf의 인덱스를 c_mask로 취득\n",
        "                c_mask = conf_scores[cl].gt(self.conf_thresh)\n",
        "                # gt는 Greater than을 의미. gt로 임곗값이 넘으면 1, 이하는 0\n",
        "                # conf_scores:torch.Size([21, 8732])\n",
        "                # c_mask:torch.Size([8732])\n",
        "\n",
        "                # scores는 torch.Size([임곗값을 넘은 BBox 수])\n",
        "                scores = conf_scores[cl][c_mask]\n",
        "\n",
        "                # 임곗값을 넘은 conf가 없는 경우, scores=[]는 아무것도 하지 않음.\n",
        "                if scores.nelement() == 0:  # nelement로 요소 수의 합계를 구함\n",
        "                    continue\n",
        "\n",
        "                # c_mask를 decoded_boxes에 적용할 수 있도록 크기 변경\n",
        "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n",
        "                # l_mask:torch.Size([8732, 4])\n",
        "\n",
        "                # l_mask를 decoded_boxes로 적용\n",
        "                boxes = decoded_boxes[l_mask].view(-1, 4)\n",
        "                # decoded_boxes[l_mask]로 1차원이 되기 때문에\n",
        "                # view에서 (임곗값을 넘은 BBox 수, 4) 크기로 바꿈\n",
        "\n",
        "                # 3. Non-Maximum Suppression을 실시하여 중복되는 BBox 제거\n",
        "                ids, count = nm_suppression(\n",
        "                    boxes, scores, self.nms_thresh, self.top_k)\n",
        "                # ids：conf의 내림차순으로 Non-Maximum Suppression을 통과한 index 작성\n",
        "                # count：Non-Maximum Suppression를 통과한 BBox 수\n",
        "\n",
        "                # output에 Non-Maximum Suppression를 뺀 결과 저장\n",
        "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1),\n",
        "                boxes[ids[:count]]), 1)\n",
        "\n",
        "        return output  # torch.Size([1, 21, 200, 5])"
      ],
      "metadata": {
        "id": "SnCq6A_zgFsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽MultiBoxLoss**\n",
        "    SSD의 손실함수 클래스 구현"
      ],
      "metadata": {
        "id": "hbDEUiKQmXRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSD의 손실함수 클래스 \"\"\"\n",
        "\n",
        "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.jaccard_thresh = jaccard_thresh  # 0.5 match 함수의 jaccard 계수의 임계치\n",
        "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Mining의 음과 양 비율\n",
        "        self.device = device  # 계산 device(CPU | GPU)\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        손실함수 계산\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : SSD net의 훈련 시 출력 (tuple)\n",
        "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, \n",
        "            8732, 21]), dbox_list=torch.Size [8732,4])。\n",
        "        targets : [num_batch, num_objs, 5]\n",
        "            5는 정답인 어노테이션 정보[xmin, ymin, xmax, ymax, label_ind]\n",
        "        Returns\n",
        "        -------\n",
        "        loss_l : 텐서\n",
        "            loc의 손실 값\n",
        "        loss_c : 텐서\n",
        "            conf의 손실 값\n",
        "        \"\"\"\n",
        "\n",
        "        # SSD의 출력이 튜플로 되어 있어 개별적으로 분리함\n",
        "        loc_data, conf_data, dbox_list = predictions\n",
        "\n",
        "        # 요소 수를 파악\n",
        "        num_batch = loc_data.size(0)  # 미니 배치 크기\n",
        "        num_dbox = loc_data.size(1)  # DBox의 수 = 8732\n",
        "        num_classes = conf_data.size(2)  # 클래스 수= 21\n",
        "\n",
        "        # 손실 계산에 사용할 것을 저장하는 변수 작성\n",
        "        # conf_t_label：각 DBox에 가장 가까운 정답 BBox의 라벨을 저장 \n",
        "        # loc_t: 각 DBox에 가장 가까운 정답 BBox의 위치 정보 저장 \n",
        "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
        "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
        "\n",
        "        # loc_t와 conf_t_label에 \n",
        "        # DBox와 정답 어노테이션 targets를 amtch한 결과 덮어쓰기\n",
        "        for idx in range(num_batch):  # 미니 배치 루프\n",
        "\n",
        "            # 현재 미니 배치의 정답 어노테이션 BBox와 라벨 취득\n",
        "            truths = targets[idx][:, :-1].to(self.device)  # BBox\n",
        "            # 라벨 [물체1 라벨, 물체2 라벨, ...]\n",
        "            labels = targets[idx][:, -1].to(self.device)\n",
        "\n",
        "            # 디폴트 박스를 새로운 변수로 준비\n",
        "            dbox = dbox_list.to(self.device)\n",
        "\n",
        "            # match 함수를 실행하여 loc_t와 conf_t_label 내용 갱신\n",
        "            # loc_t: 각 DBox에 가장 가까운 정답 BBox 위치 정보가 덮어써짐.\n",
        "            # conf_t_label：각 DBox에 가장 가까운 정답 BBox 라벨이 덮어써짐.\n",
        "            # 단, 가장 가까운 BBox와 jaccard overlap이 0.5보다 작은 경우,\n",
        "            # 정답 BBox의 라벨 conf_t_label은 배경 클래스 0으로 한다.\n",
        "            variance = [0.1, 0.2]\n",
        "            # 이 variance는 DBox에서 BBox로 보정 계산할 때 사용하는 식의 계수\n",
        "            match(self.jaccard_thresh, truths, dbox,\n",
        "                  variance, labels, loc_t, conf_t_label, idx)\n",
        "\n",
        "        # ----------\n",
        "        # 위치 손실 : loss_l을 계산\n",
        "        # Smooth L1 함수로 손실 계산. 단, 물체를 발견한 DBox의 오프셋만 계산\n",
        "        # ----------\n",
        "        # 물체를 감지한 BBox를 꺼내는 마스크 작성\n",
        "        pos_mask = conf_t_label > 0  # torch.Size([num_batch, 8732])\n",
        "\n",
        "        # pos_mask를 loc_data 크기로 변형\n",
        "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
        "\n",
        "        # Positive DBox의 loc_data와 지도 데이터 loc_t 취득\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "\n",
        "        # 물체를 발견한 Positive DBox의 오프셋 정보 loc_t의 손실(오차)를 계산\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # ----------\n",
        "        # 클래스 예측의 손실 : loss_c를 계산\n",
        "        # 교차 엔트로피 오차 함수로 손실 계산. 단 배경 클래스가 정답인 DBox가 압도적으로 많으므로,\n",
        "        # Hard Negative Mining을 실시하여 물체 발견 DBox 및 배경 클래스 DBox의 비율이 1:3이 되도록 한다.\n",
        "        # 배경 클래스 DBox로 예상한 것 중 손실이 적은 것은 클래스 예측 손실에서 제외\n",
        "        # ----------\n",
        "        batch_conf = conf_data.view(-1, num_classes)\n",
        "\n",
        "        # 클래스 예측의 손실함수 계산(reduction='none'으로 하여 합을 취하지 않고 차원 보존)\n",
        "        loss_c = F.cross_entropy(\n",
        "            batch_conf, conf_t_label.view(-1), reduction='none')\n",
        "\n",
        "        # -----------------\n",
        "        # Negative DBox중 Hard Negative Mining으로 \n",
        "        # 추출하는 것을 구하는 마스크 작성\n",
        "        # -----------------\n",
        "\n",
        "        # 물체를 발견한 Positive DBox의 손실을 0으로 한다.\n",
        "        # (주의) 물체는 label이 1 이상, 라벨 0은 배경을 의미\n",
        "        num_pos = pos_mask.long().sum(1, keepdim=True)  # 미니 배치별 물체 클래스 예측 수\n",
        "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
        "        loss_c[pos_mask] = 0  # 물체를 발견한 DBox는 손실 0으로 한다.\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        # 각 DBox 손실의 크기 loss_c 순위 idx_rank를 구함\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "\n",
        "        #  (주의) 구현된 코드는 특수하며 직관적이지 않음.\n",
        "        # 위 두 줄의 요점은 각 DBox에 대해 손실 크기가 몇 번째인지의 정보를 \n",
        "        # idx_rank 변수로 빠르게 얻는 코드이다.\n",
        "        \n",
        "        # DBox의 손실 값이 큰 쪽부터 내림차순으로 정렬하여, \n",
        "        # DBox의 내림차순의 index를 loss_idx에 저장한다.\n",
        "        # 손실 크기 loss_c의 순위 idx_rank를 구한다.\n",
        "        # 내림차순이 된 배열 index인 loss_idx를 0부터 8732까지 오름차순으로 다시 정렬하기 위하여\n",
        "        # 몇 번째 loss_idx의 인덱스를 취할 지 나타내는 것이 idx_rank이다.\n",
        "        # 예를 들면 idx_rank 요소의 0번째 = idx_rank[0]을 구하는 것은 loss_idx의 값이 0인 요소,\n",
        "        # 즉 loss_idx[?] =0은 원래 loss_c의 요소 0번째는 내림차순으로 정렬된 loss_idx의 \n",
        "        # 몇 번째입니까? 를구하는 것이 되어 결과적으로, \n",
        "        # ? = idx_rank[0]은 loss_c의 요소 0번째가 내림차순으로 몇 번째인지 나타냄\n",
        "\n",
        "        # 배경 DBox의 수 num_neg를 구한다. HardNegative Mining으로 \n",
        "        # 물체 발견 DBox으 ㅣ수 num_pos의 세 배 (self.negpos_ratio 배)로 한다.\n",
        "        # DBox의 수를 초과한 경우에는 DBox의 수를 상한으로 한다.\n",
        "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
        "\n",
        "        # idx_rank에 각 DBox의 손실 크기가 위에서부터 몇 번째인지 저장되었다.\n",
        "        # 배경 DBox의 수 num_neg보다 순위가 낮은(손실이 큰) DBox를 취하는 마스크 작성\n",
        "        # torch.Size([num_batch, 8732])\n",
        "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
        "\n",
        "        # -----------------\n",
        "        # (종료) 지금부터 Negative DBox 중 Hard Negative Mining으로 추출할 것을 구하는 마스크를 작성\n",
        "        # -----------------\n",
        "\n",
        "        # 마스크 모양을 고쳐 conf_data에 맞춘다\n",
        "        # pos_idx_mask는 Positive DBox의 conf를 꺼내는 마스크이다.\n",
        "        # neg_idx_mask는 Hard Negative Mining으로 추출한 Negative DBox의 conf를 꺼내는 마스크이다.\n",
        "        # pos_mask：torch.Size([num_batch, 8732])\n",
        "        # --> pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
        "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
        "\n",
        "        # conf_data에서 pos와 neg만 꺼내서 conf_hnm으로 한다. \n",
        "        # 형태는 torch.Size([num_pos+num_neg, 21])\n",
        "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
        "                             ].view(-1, num_classes)\n",
        "        # gt는 greater than (>)의 약칭. mask가 1인 index를 꺼낸다\n",
        "        # pos_idx_mask+neg_idx_mask는 덧셈이지만 index로 mask를 정리할 뿐임.\n",
        "        # pos이든 neg이든 마스크가 1인 것을 더해 하나의 리스트로 만들어 이를 gt로 췯그한다.\n",
        "\n",
        "        # 마찬가지로 지도 데이터인 conf_t_label에서 pos와 neg만 꺼내, conf_t_label_hnm 으로 \n",
        "        # torch.Size([pos+neg]) 형태가 된다\n",
        "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
        "\n",
        "        # confidence의 손실함수 계산(요소의 합계=sum을 구함)\n",
        "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
        "\n",
        "        # 물체를 발견한 BBox의 수 N (전체 미니 배치의 합계) 으로 손실을 나눈다.\n",
        "        N = num_pos.sum()\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "\n",
        "        return loss_l, loss_c"
      ],
      "metadata": {
        "id": "qgSbZowKgJhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ◽ **데이터셋 구현**"
      ],
      "metadata": {
        "id": "y0NUHzcAmdmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
        "\n",
        "# Dataset 작성\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "              'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
        "              'sheep','sofa','train','tvmonitor']\n",
        "\n",
        "color_mean = (104, 117, 123) # BGR 색 평균 값 \n",
        "\n",
        "input_size = 300 # input 크기를 300*300 으로 설정\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, \n",
        "phase='train', transform=DataTransform(\n",
        "                            input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val',\n",
        "\t\t\t\t\t\ttransform=DataTransform(input_size, color_mean), \n",
        "                            transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "# dataloader 작성\n",
        "batch_size=32\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "                            train_dataset, batch_size=batch_size, \n",
        "                            shuffle=True, \n",
        "                       \tcollate_fn = od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "                            val_dataset, batch_size = batch_size,\n",
        "                            shuffle=False, collate_fn= od_collate_fn)\n",
        "\n",
        "# 사전 오브젝트로 정리\n",
        "dataloaders_dict = {'train' : train_dataloader, 'val' : val_dataloader}"
      ],
      "metadata": {
        "id": "-sIz6sIHgMZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽ 신경망 구현**"
      ],
      "metadata": {
        "id": "-W3Jirb6m8Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SSD300 설정\n",
        "ssd_cfg = {\n",
        "    'num_classes' : 21,\n",
        "    'input_size' : 300,\n",
        "    'bbox_aspect_num' : [4, 6, 6, 6, 4, 4],  # 출력할 DBox의 화면 비 종류\n",
        "    'feature_maps' : [38, 19, 10, 5, 3, 1],  # 각 source 화상 크기\n",
        "    'steps' : [8, 16, 32, 64, 100, 300],\n",
        "    'min_sizes' : [30, 60, 111, 162, 213, 264] , # DBox 크기(최소)\n",
        "    'max_sizes' : [60, 111, 162, 213, 264, 315], # DBox 크기(최대)\n",
        "    'aspect_ratios' : [[2], [2,3], [2,3], [2,3], [2], [2]]\n",
        "}\n",
        "\n",
        "# SSD 네트워크 모델\n",
        "net = SSD(phase='train', cfg=ssd_cfg)\n",
        "\n",
        "# SSD 초기 가중치 설정\n",
        "# ssd의 vgg에 가중치 로드\n",
        "vgg_weights = torch.load('./pytorch_advanced/objectdetection/weights/vgg16_reducedfc.pth')\n",
        "net.vgg.load_state_dict(vgg_weights)\n",
        "\n",
        "# ssd의 기타 네트워크 가중치는 He의 초기치로 초기화\n",
        "\n",
        "def weights_init(m) :\n",
        "    if isinstance(m, nn.Conv2d) :\n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        if m.bias is not None : # bias 항이 있는 경우\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "            \n",
        "# He의 초기치 적용\n",
        "net.extras.apply(weights_init)\n",
        "net.loc.apply(weights_init)\n",
        "net.conf.apply(weights_init)\n",
        "\n",
        "# GPU를 사용할 수 있는지 확인\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'사용 중인 장치 : {device}')\n",
        "\n",
        "print('네트워크 설정 완료 : 학습된 가중치를 로드했습니다.')"
      ],
      "metadata": {
        "id": "E0DaZOtBgPyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **◽ 손실함수 구현 및 학습**"
      ],
      "metadata": {
        "id": "gUhtM1Ecm_Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수 설정\n",
        "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n",
        "\n",
        "# 최적화 기법 선정\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, \n",
        "                             momentum = 0.9, weight_decay=5e-4)\n",
        "\n",
        "# 학습 및 검증 실시\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs) :\n",
        "    \n",
        "    # GPU 사용 확인\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'사용 중인 장치 : {device}')\n",
        "    \n",
        "    # 네트워크를 gpu로\n",
        "    net.to(device)\n",
        "    \n",
        "    # 네트워크가 어느정도 고정되면 고속화\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "    # 반복자 카운터 설정\n",
        "    iteration = 1\n",
        "    epoch_train_loss = 0.0 # 에포크 손실 합\n",
        "    epoch_val_loss = 0.0 # 에포크 손실 합\n",
        "    logs = []\n",
        "    \n",
        "    \n",
        "    # 에폭 루프\n",
        "    for epoch in range(num_epochs+1) :\n",
        "        \n",
        "        # 시작 시간 저장\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        \n",
        "        print('-' * 20)\n",
        "        print(f'Epoch : {epoch+1}/{num_epochs}')\n",
        "        print('-' * 20)\n",
        "        \n",
        "        # 에폭 별 훈련 및 검증 루프\n",
        "        for phase in ['train', 'val'] :\n",
        "            if phase == 'train' :\n",
        "                net.train() # 모델을 훈련 모드로\n",
        "                print('(train)')\n",
        "            else :\n",
        "                if ((epoch+1) % 10  == 0 ) :\n",
        "                    net.eval() # 모델을 검증 모드로\n",
        "                    print('-' * 20)\n",
        "                    print('(val)')\n",
        "                else :\n",
        "                    # 검증은 10 회 중 1회만 실시\n",
        "                    continue\n",
        "                    \n",
        "            # 데이터 로더에서 미니 배치씩 꺼내 루프\n",
        "            for images, targets in dataloaders_dict[phase] :\n",
        "                # GPU를 사용할 수 있으면 GPU에 데이터를 보낸다.\n",
        "                images = images.to(device)\n",
        "                # 리스트 각 요소의 텐서를 GPUㄹ\n",
        "                targets = [ann.to(device) for ann in targets]\n",
        "                # 옵티마이저 초기화\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                \n",
        "                # 순전파 계산\n",
        "                with torch.set_grad_enabled(phase=='train') :\n",
        "                    # 순전파 계산\n",
        "                    outputs = net(images)\n",
        "                    loss_l, loss_c = criterion(outputs, targets) \n",
        "                    loss = loss_l + loss_c\n",
        "                    \n",
        "                    # 훈련 시에는 역전파\n",
        "                    if phase == 'train' :\n",
        "                        loss.backward() \n",
        "                        \n",
        "                        # 경사가 너무 커지면 계산이 부정확해 clip에서 최대경사 2.0에 고정\n",
        "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
        "                        \n",
        "                        optimizer.step() # 파라미터 갱신\n",
        "                        \n",
        "                        if (iteration % 10 == 0) : # 10 iter에 한 번 손실 표시\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print(f'반복 {iteration} \n",
        "                            || Loss : {loss.item():.4f} || 10 iter : {duration:.4f} \\\n",
        "                                    sec.')\n",
        "\n",
        "                            t_iter_start =time.time()\n",
        "                            \n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "                        \n",
        "                    # 검증 시\n",
        "                    else :\n",
        "                        epoch_val_loss += loss.item()\n",
        "                        \n",
        "            \n",
        "            # epoch의 phase당 손실과 정답률\n",
        "            t_epoch_finish = time.time()\n",
        "            print('-' * 20)\n",
        "            print(f'epoch {epoch+1} || Epoch_Train_loss : {epoch_train_loss:.4f} \n",
        "            || Epoch_val_loss : {epoch_val_loss:.4f}')\n",
        "            \n",
        "            print(f'timer : {t_epoch_finish - t_epoch_start:.4f} sec')\n",
        "            \n",
        "            # 로그 저장\n",
        "            loc_epoch = {'epoch' : epoch + 1,\n",
        "                        'train_loss' : epcoh_train_loss,\n",
        "                        'val_loss' : epoch_val_loss}\n",
        "            los.append(los_epoch)\n",
        "            df = pd.DataFrame(logs)\n",
        "            df.to_csv('log_output.csv')\n",
        "            \n",
        "            epoch_train_loss = 0.0 # 에폭 손실 합\n",
        "            epoch_val_loss = 0.0 # 에폭 손실 합\n",
        "            \n",
        "            # 네트워크 저장\n",
        "            if ((epoch + 1) % 10 == 0 ) :\n",
        "                torch.save(net.start_dict(), \n",
        "                'pytorch_advanced/objectdetection/weights/ssd300_' + str(epoch+1) + '.pth')\n",
        "\n",
        "                \n",
        "# 학습 및 검증 실시\n",
        "num_epochs = 50\n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs = num_epochs)"
      ],
      "metadata": {
        "id": "iLrJMrzsgSPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**◽ 추론 실시**\n"
      ],
      "metadata": {
        "id": "KTMVyEfonFbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "              'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
        "              'sheep','sofa','train','tvmonitor']\n",
        "\n",
        "# SSD 300 설정\n",
        "\n",
        "ssd_cfg = {\n",
        "    'num_classes' : 21,\n",
        "    'input_size' : 300,\n",
        "    'bbox_aspect_num' : [4, 6, 6, 6, 4, 4], # 출력할 DBox 화면비 종류\n",
        "    'feature_maps' : [38, 19, 10, 5, 3, 1], # 각 source의 화상 크기\n",
        "    'steps' : [8, 16, 32, 64, 100, 300], # DBox 크기 결정\n",
        "    'min_sizes' : [30, 60, 111, 162, 213, 264], # DBox 크기 결정\n",
        "    'max_sizes' : [60, 111, 162, 213, 264, 315], # DBox 크기 결정\n",
        "    'aspect_ratios' : [[2], [2,3], [2,3], [2,3], [2], [2]],\n",
        "}\n",
        "\n",
        "# SSD 네트워크 모델\n",
        "net = SSD(phase='inference', cfg=ssd_cfg)\n",
        "\n",
        "# SSD의 학습된 가중치 설정\n",
        "# net_weights = torch.load('./pytorch_advanced/objectdetection/weights/ssd300_50.pth',\n",
        "#                         map_location={'cuda:0' : 'cpu'})\n",
        "\n",
        "net_weights = torch.load('./pytorch_advanced/objectdetection/weights/ssd300_mAP_77.43_v2.pth',\n",
        "                        map_location={'cuda:0' : 'cpu'})\n",
        "\n",
        "net.load_state_dict(net_weights)\n",
        "\n",
        "print('네트워크 설정 완료 : 학습 가중치를 로드했습니다.')"
      ],
      "metadata": {
        "id": "VLZwNzZ5gT7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 화상 읽기\n",
        "\n",
        "image_file_path = './pytorch_advanced/objectdetection/data/cowboy-757575_640.jpg'\n",
        "img = cv2.imread(image_file_path) # [높이][폭][색BGR]\n",
        "height, width, channels = img.shape # 이미지 크기 취득\n",
        "\n",
        "# 2. 원본 화상 표시\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 3. 전처리 클래스 작성\n",
        "color_mean = (104, 117, 123) # BGR 색 평균값\n",
        "input_size= 300 # 화상 크기를 300*300 으로 설정\n",
        "transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "# 4. 전처리\n",
        "phase = 'val'\n",
        "img_transformed, boxes, labels = transform(img, phase, '', '' ) # 어노테이션은 없어 \"\"으로 설정\n",
        "img = torch.from_numpy(img_transformed[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "# 5. SSD로 예측\n",
        "net.eval() # 네트워크를 추론 모드로\n",
        "\n",
        "x = img.unsqueeze(0) # 미니 배치화 : torch.Size([1, 3, 300, 300])\n",
        "\n",
        "detections = net(x)\n",
        "\n",
        "print(detections.shape)\n",
        "print(detections)"
      ],
      "metadata": {
        "id": "98B16FObgV0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch_advanced/2_objectdetection/utils/ssd_predict_show.py\n",
        "class SSDPredictShow(): \n",
        "    \"\"\"SSD에서의 예측과 이미지 표시를 한꺼번에 하는 클래스\"\"\"\n",
        "\n",
        "    def __init__(self, eval_categories, net):\n",
        "        self.eval_categories = eval_categories  # 클래스명\n",
        "        self.net = net  # SSD 네트워크\n",
        "\n",
        "        color_mean = (104, 117, 123)  # (BGR)색상의 평균치\n",
        "        input_size = 300  # 사진의 input 크기를 300 × 300으로 한다.\n",
        "        self.transform = DataTransform(input_size, color_mean)  # 전처리 클래스\n",
        "\n",
        "    def show(self, image_file_path, data_confidence_level):\n",
        "        \"\"\"\n",
        "        물체 검출의 예측 결과를 표시하는 함수.\n",
        "        Parameters\n",
        "        ----------\n",
        "        image_file_path:  str\n",
        "            이미지 파일 경로\n",
        "        data_confidence_level: float\n",
        "            예측으로 발견할 수 있는 확신도의 역치\n",
        "        Returns\n",
        "        -------\n",
        "        없음. rgb_img에 물체검출결과가 더해진 화상이 나타난다.\n",
        "        \"\"\"\n",
        "        rgb_img, predict_bbox, pre_dict_label_index, scores = self.ssd_predict(\n",
        "            image_file_path, data_confidence_level)\n",
        "\n",
        "        self.vis_bbox(rgb_img, bbox=predict_bbox, label_index=pre_dict_label_index,\n",
        "                      scores=scores, label_names=self.eval_categories)\n",
        "\n",
        "    def ssd_predict(self, image_file_path, data_confidence_level=0.5):\n",
        "        \"\"\"\n",
        "        SSD에서 예측시키는 함수.\n",
        "        Parameters\n",
        "        ----------\n",
        "        image_file_path:  strt\n",
        "            이미지 파일 경로\n",
        "        dataconfidence_level: float\n",
        "            예측으로 발견할 수 있는 확신도의 역치\n",
        "        Returns\n",
        "        -------\n",
        "        rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores\n",
        "        \"\"\"\n",
        "\n",
        "        # rgb 이미지 데이터 가져오기\n",
        "        img = cv2.imread(image_file_path)  # [높이] [폭] [색상BGR]\n",
        "        height, width, channels = img.shape  # 이미지 크기 가져오기\n",
        "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # 화상 전처리\n",
        "        phase = \"val\"\n",
        "        img_transformed, boxes, labels = self.transform(\n",
        "            img, phase, \"\", \"\")  # 어노테이션이 존재하지 않기 때문에 \"\"로 한다\n",
        "        img = torch.from_numpy(\n",
        "            img_transformed[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        # SSD로 예측\n",
        "        self.net.eval()  # 네트워크를 추론 모드로\n",
        "        x = img.unsqueeze(0)  # 미니 배치 화：torch.Size([1, 3, 300, 300])\n",
        "\n",
        "        detections = self.net(x)\n",
        "        # detections의 형태는 torch.Size (1, 21, 200, 5]) ※200은 top_k 의 값\n",
        "\n",
        "        # confidence_level 이 기준 이상을 꺼냄\n",
        "        predict_bbox = []\n",
        "        pre_dict_label_index = []\n",
        "        scores = []\n",
        "        detections = detections.cpu().detach().numpy()\n",
        "\n",
        "        # 조건 이상의 값을 추출\n",
        "        find_index = np.where(detections[:, 0:, :, 0] >= data_confidence_level)\n",
        "        detections = detections[find_index]\n",
        "        for i in range(len(find_index[1])):  # 추출한 물체 수만큼 루프\n",
        "            if (find_index[1][i]) > 0:  # 배경 클래스가 아닌 것\n",
        "                sc = detections[i][0]  # 확신도\n",
        "                bbox = detections[i][1:] * [width, height, width, height]\n",
        "                # find_index는 미니뱃지 수, 클래스, top의 tuple\n",
        "                lable_ind = find_index[1][i]-1\n",
        "                ## (주석)\n",
        "                # 배경반이 0이므로 1을 뺀다\n",
        "\n",
        "                # 반환 값 리스트에 추가\n",
        "                predict_bbox.append(bbox)\n",
        "                pre_dict_label_index.append(lable_ind)\n",
        "                scores.append(sc)\n",
        "\n",
        "        return rgb_img, predict_bbox, pre_dict_label_index, scores\n",
        "\n",
        "    def vis_bbox(self, rgb_img, bbox, label_index, scores, label_names):\n",
        "        \"\"\"\n",
        "        물체 검출의 예측 결과를 화상으로 표시시키는 함수.\n",
        "        Parameters\n",
        "        ----------\n",
        "        rgb_img: rgb이미지\n",
        "        대상 이미지 데이터\n",
        "        bbox : list\n",
        "        물체의 비폭스 목록\n",
        "        label _ index : list\n",
        "        물체의 라벨에 대한 인덱스\n",
        "        scores : list\n",
        "        물체의 확신도\n",
        "        label _ names : list\n",
        "        라벨명의 배열\n",
        "        Returns\n",
        "        -------\n",
        "        없음. rgb_img에 물체검출결과가 더해진 화상이 나타난다.\n",
        "        \"\"\"\n",
        "        # 테두리의 색 설정\n",
        "        num_classes = len(label_names)  # 클래스 수 (배경 제외)\n",
        "        colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n",
        "\n",
        "        # 이미지 표시\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(rgb_img)\n",
        "        currentAxis = plt.gca()\n",
        "\n",
        "        # BBox의 루프\n",
        "        for i, bb in enumerate(bbox):\n",
        "\n",
        "            # 라벨 명\n",
        "            label_name = label_names[label_index[i]]\n",
        "            color = colors[label_index[i]]  # クラスごとに別の色の枠を与える\n",
        "\n",
        "            # 테두리에 붙이는 라벨 // 예) person : 0.72\n",
        "            if scores is not None:\n",
        "                sc = scores[i]\n",
        "                display_txt = '%s: %.2f' % (label_name, sc)\n",
        "            else:\n",
        "                display_txt = '%s: ans' % (label_name)\n",
        "\n",
        "            # 테두리 좌표\n",
        "            xy = (bb[0], bb[1])\n",
        "            width = bb[2] - bb[0]\n",
        "            height = bb[3] - bb[1]\n",
        "\n",
        "            # 직사각형을 그림\n",
        "            currentAxis.add_patch(plt.Rectangle(\n",
        "                xy, width, height, fill=False, edgecolor=color, linewidth=2))\n",
        "\n",
        "            # 직사각형 틀의 왼쪽 상단에 라벨을 그림\n",
        "            currentAxis.text(xy[0], xy[1], display_txt, bbox={\n",
        "                             'facecolor': color, 'alpha': 0.5})"
      ],
      "metadata": {
        "id": "m3_AYBwXgYQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 화상 예측\n",
        "\n",
        "img_file_path = './pytorch_advanced/objectdetection/data/cowboy-757575_640.jpg'\n",
        "\n",
        "# 예측 및 결과를 화상으로 그린다\n",
        "ssd = SSDPredictShow(eval_categories=voc_classes, net=net)\n",
        "ssd.show(img_file_path, data_confidence_level=0.6)"
      ],
      "metadata": {
        "id": "FGEDj5PWgZ63"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}